We present a novel architecture for safely integrating Large Language Models (LLMs) into interactive game engines, allowing players to "program" new behaviors using natural language. Our framework mitigates risks by using an LLM to translate commands into a constrained Domain-Specific Language (DSL), which configures a custom Entity-Component-System (ECS) at runtime. We evaluated this system in a 2D spell-crafting game prototype by experimentally assessing models from the *Gemini*, *GPT*, and *Claude* families with various prompting strategies. A validated LLM judge qualitatively rated the outputs, showing that while larger models better captured creative intent, the optimal prompting strategy is task-dependent: Chain-of-Thought improved creative alignment, while few-shot examples were necessary to generate more complex DSL scripts. This work offers a validated LLM-ECS pattern for emergent gameplay and a quantitative performance comparison for developers.

A playable build of the developed software artifact is provided for Windows x64. Playing requires that your own API key be provided; instructions to obtain a free API key from Google are provided along with the game.

The analysis code is provided as a mixture of Python and R, along with the generated data. The Python code's dependencies can be installed quickly with the included requirements.txt file, or by installing the packages listed in the main code notebook. Python 3.11 or later is required. R was used via RStudio, but any R installation should work. A text file containing all R outputs used for the final report is also provided.

Read the paper on ArXiv: https://doi.org/10.48550/arXiv.2510.16952
